{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Intro](#intro)  \n",
    "[Setup](#setup)  \n",
    "[Load Data](#loaddata)  \n",
    " - [Load Sales History](#saleshistory)  \n",
    " - [Load Summary Table](#summarytable)  \n",
    " - [Merge](#merge)\n",
    " \n",
    "[Exploration](#explore)  \n",
    "[Export](#export)  \n",
    "[Feature Engineering](#features)  \n",
    "[Modeling](#model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# Intro\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to forecast short-term (up to 7 days out) price behaviour for all of the sneakers in my dataset.  \n",
    "In this notebook I'll load & clean the data, and run a regression model to make predictions.  \n",
    "The results of this analysis will be written to a csv file for use in a flask app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "# Setup\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports and options\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pickle\n",
    "import re\n",
    "from random import randint\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "from seaborn import plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Pandas display options\n",
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.float_format = '{:20,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hist(df, column, title=None, xlabel='', pct=False, range=None):\n",
    "    multiplier = 1\n",
    "    if pct == True:\n",
    "        multiplier = 100\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    plt.hist(df[column]*multiplier, bins=50, range=range)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel(xlabel if xlabel else '')\n",
    "    plt.title(column if not title else title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_box(df, xcolumn, title=None, ylabel='', xlabel='', pct=False):\n",
    "    multiplier = 1\n",
    "    if pct == True:\n",
    "        multiplier = 100\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = sns.boxplot(x=df[xcolumn]*multiplier, showfliers=False, palette=\"deep\")\n",
    "    ax.set(title=xcolumn if not title else title)\n",
    "    ax.set(xlabel=xcolumn if not xlabel else xlabel )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_line(df, xcolumn, ycolumn, title=None, ylabel='', xlabel='Date', pct=False):\n",
    "    multiplier = 1\n",
    "    if pct == True:\n",
    "        multiplier = 100\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    plt.plot(df[xcolumn], df[ycolumn]*multiplier, linewidth=1)\n",
    "    plt.title(xcolumn if not title else title)\n",
    "    plt.ylabel(ylabel if ylabel else '')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.xticks(rotation=45)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_bar(df, xcolumn, ycolumn, title=None, xlabel='', ylabel='', pct=False):\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    if pct == True:\n",
    "        df[ycolumn] = df[ycolumn] * 100\n",
    "    ax = df.groupby(xcolumn)[ycolumn].mean().plot(kind = 'bar')\n",
    "    plt.xlabel(xlabel if xlabel else '')\n",
    "    plt.ylabel(ylabel if ylabel else '')\n",
    "    plt.title(xcolumn if not title else title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define general helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_columns(df, columns):\n",
    "    '''Drops a list of columns in a dataframe'''\n",
    "    for c in columns:\n",
    "        df.drop(c, axis = 1, inplace = True)\n",
    "        \n",
    "def get_percent(part, whole):\n",
    "    return round(100 * float(part)/float(whole), 2)\n",
    "\n",
    "def unique_list(l):\n",
    "    '''makes a unique list'''\n",
    "    ulist = []\n",
    "    [ulist.append(x) for x in l if x not in ulist]\n",
    "    return ulist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loaddata'></a>\n",
    "# Load Data\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Descriptions:\n",
    "***sales_history.pkl*** - history by shoe by sale   \n",
    "***shoe_file.json*** - descriptive information by shoe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleshistory'></a>\n",
    "## Load Sales History\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open the cleaned sales_history data\n",
    "with open('data/sales_history.pkl', 'rb') as picklefile:\n",
    "    sales_history = pickle.load(picklefile)\n",
    "    \n",
    "original_sales_length = len(sales_history)\n",
    "print original_sales_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleanup functions\n",
    "def extract_date_part(df):\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = df.index.day\n",
    "    df['hour'] = df.index.hour\n",
    "    df['sale_date'] = df.index.date\n",
    "    \n",
    "def price_to_float(df, cols):\n",
    "    '''strips out non-numericals and convert to floats'''\n",
    "    for c in cols:\n",
    "        df[c] = df[c].replace({'\\D': ''}, regex=True)\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "        \n",
    "def shoe_brand(x):\n",
    "    \"\"\"get brand of shoe based on line\"\"\"\n",
    "    if x in adidas_list:\n",
    "        x = \"adidas\"\n",
    "    elif x in nike_list:\n",
    "        x = \"nike\"\n",
    "    return x\n",
    "\n",
    "def clean_sizes(x):\n",
    "    \"\"\"convert shoe sizes into floats\"\"\"\n",
    "    if \"K\" in x:\n",
    "        x = 1\n",
    "    elif \"Y\" in x:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = re.sub(r'[a-zA-Z-/]', ' ', x)\n",
    "        x_list = [float(x) for x in x.strip().split()]\n",
    "        x = sum(x_list) / float(len(x_list))\n",
    "        x = round(x, 2)\n",
    "    return x\n",
    "\n",
    "def core_fringe(x):\n",
    "    \"\"\"converts shoe size into 1 for fringe or 0 for core sizing\"\"\"\n",
    "    if x < 7.5:\n",
    "        x = 1\n",
    "    elif x > 14:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maintain lists of lines within shoe brands\n",
    "adidas_list = ['y-3', 'y3', 'adidas','yeezy', 'nmd']\n",
    "nike_list = ['jordan','nike','foamposite','air','kobe','lebron','kd','sf','kyrie','lunar','cavs',\n",
    "                 'pg','lebron','flyknit']\n",
    "other_list = ['reebok', 'new', 'asics','vans','ua','puma','saucony','diadora','timberland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a duplicate column for datetime and shoe name\n",
    "sales_history['sale_date_time'] = sales_history.index\n",
    "sales_history['shoe_name'] = sales_history['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean up the sales_history dataframe \n",
    "extract_date_part(sales_history)\n",
    "price_to_float(sales_history, ['sale_price'])\n",
    "drop_columns(sales_history, ['date_time'])\n",
    "\n",
    "# Extract the line from the shoe name and add the brand column\n",
    "sales_history['line'] = sales_history['name'].str.split().str.get(0).str.lower()\n",
    "sales_history['brand'] = sales_history['line'].apply(lambda x: shoe_brand(x))\n",
    "\n",
    "# Add a column for shoe size range\n",
    "sales_history['size_number'] = sales_history['shoe_size'].apply(lambda x: clean_sizes(x))\n",
    "sales_history['shoe_size'] = sales_history['size_number'].apply(lambda x: core_fringe(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_history.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summarytable'></a>\n",
    "## Load Summary Table\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = pd.read_json('data/shoe_file.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orig_summary_length = len(summary)\n",
    "print orig_summary_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleanup functions for the summary table\n",
    "def consolidate_colors(x):\n",
    "    '''Simplifies shoe colors into basic colors(black, white, etc) or other'''\n",
    "    things_to_replace = [\"/\",\"-\",\"core\",\"true\",\"pirate\",\"fierce\",\"turtledove\",\"varsity\",\n",
    "                         \"fire\",\"royal\",\"gym\",\"wolf\",\"infared\",\"23\",\"metallic\",\"bone\",\n",
    "                         \"dark\",\"university\",\"concord\",\"anthracite\",\"coin\",\"cool\",\"cement\",\"   \"]\n",
    "    for thing in things_to_replace:\n",
    "        x = x.replace(thing,\" \")\n",
    "        x = x.strip()\n",
    "        \n",
    "    x = ' '.join(sorted(unique_list(x.split())))\n",
    "    \n",
    "    if x == \"black white\":\n",
    "        x = \"basic\"\n",
    "    elif x == \"black\":\n",
    "        x = \"basic\"\n",
    "    elif x == \"white\":\n",
    "        x = \"basic\"\n",
    "    else:\n",
    "        x = \"other\"\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "summary.rename(\n",
    "    columns={'release date':'release_date', 'original retail':'original_retail','style':'style_code'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace string values in price columns\n",
    "price_to_float(summary,['num_sales', 'highest_bid', 'lowest_ask', 'original_retail'])\n",
    "\n",
    "# Convert release date to datetime format    \n",
    "summary = summary[~summary.release_date.str.contains(\"n/a\")]\n",
    "summary['release_date'] = pd.to_datetime(summary['release_date'],  format=' %m.%d.%y')\n",
    "\n",
    "# Remove newline from the name column\n",
    "summary['name'] = summary.name.str.replace(\"\\n\", \" \")\n",
    "\n",
    "# Add a simplified color column for use in later modeling\n",
    "summary['main_color'] = summary['colorway'].apply(lambda x: consolidate_colors(x))\n",
    "\n",
    "# Drop any duplicates across the whole dataframe\n",
    "summary = summary.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle list for use in the instagram scraper\n",
    "unique_names = [name.encode('ascii','ignore') for name in summary.name.unique()]\n",
    "unique_names = sorted(unique_names)\n",
    "\n",
    "with open('data/shoe_name_list.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(unique_names, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_history.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='merge'></a>\n",
    "## Merge\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the summary and sales_history dataframes\n",
    "sales_history = sales_history.merge(summary, on='name', sort=False, how='inner')\n",
    "\n",
    "# Sort the new merged table by shoe name\n",
    "sales_history.sort_values(['name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns:\n",
    "drop_list = ['lowest_ask','num_asks','num_bids','num_sales','highest_bid']\n",
    "\n",
    "drop_columns(sales_history, drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_history.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='explore'></a>\n",
    "# Exploration\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_bar(sales_history, 'month', 'sale_price', title=\"Avg. Sale Price by Month\", \n",
    "         xlabel=\"Month\", ylabel=\"Avg. Sale Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_bar(sales_history, 'size_number', 'sale_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_hist(sales_history, 'sale_price', range=[0,2000], title='Count of Sale Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='export'></a>\n",
    "# Export\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a few tables below for the web app to pull from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/final_sales_history.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(sales_history, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features'></a>\n",
    "# Feature Engineering\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add a num_sales (frequency) column\n",
    "sales_history['num_sales'] = sales_history.groupby('shoe_name')['shoe_name'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Limit everything below this to shoes with over 50 sales recorded\n",
    "print(len(sales_history))\n",
    "sales_history = sales_history[sales_history.num_sales > 50]\n",
    "print(len(sales_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregate to sales by day\n",
    "shoes = sales_history[['name', 'sale_date', 'sale_price']].groupby(['name', 'sale_date'])\n",
    "shoes = shoes.aggregate(['mean', 'count']).reset_index()\n",
    "\n",
    "# Collapse the header\n",
    "shoes.columns = shoes.columns.droplevel(0)\n",
    "\n",
    "# Convert groupby object to dataframe & name columns\n",
    "shoes = pd.DataFrame(shoes)\n",
    "shoes.columns = ['name', 'sale_date', 'sale_price', 'volume']\n",
    "sales_history = sales_history.reset_index()\n",
    "\n",
    "# add back in the other information about the shoe\n",
    "more_info = (sales_history[['name', 'main_color', 'line', 'brand', 'style_code', 'image_url',\n",
    "                           'release_date', 'original_retail', 'colorway']]\n",
    "             .drop_duplicates())\n",
    "\n",
    "shoes = shoes.merge(more_info, on = 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take the log of the sale price\n",
    "shoes['log_sale_price'] = np.log(shoes.sale_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shoes.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Overwrite sales_history - no more original sales_history dataframe anymore\n",
    "sales_history = shoes.copy()\n",
    "sales_history['shoe_name'] = sales_history['name']\n",
    "sales_history['sale_date'] = pd.to_datetime(sales_history['sale_date'])\n",
    "sales_history['time_since_release'] = sales_history['sale_date'] - sales_history['release_date']\n",
    "\n",
    "# change the time delta into a count of days since release date & log it\n",
    "sales_history['total_days_td'] = sales_history['time_since_release'].dt.total_seconds() / (24 * 60 * 60)\n",
    "sales_history['log_total_days_td'] = np.log(sales_history.total_days_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Last sale price\n",
    "sales_history['sale_lagged_1'] = sales_history.groupby(['shoe_name']).sale_price.shift(1)\n",
    "sales_history['pct_vs_last'] = sales_history['sale_price'] / sales_history['sale_lagged_1'] - 1\n",
    "# Sale price minus 2\n",
    "sales_history['sale_lagged_2'] = sales_history.groupby(['shoe_name']).sale_price.shift(2)\n",
    "sales_history['pct_vs_last_2'] = sales_history['sale_lagged_1'] / sales_history['sale_lagged_2'] - 1\n",
    "# Sale price minus 3\n",
    "sales_history['sale_lagged_3'] = sales_history.groupby(['shoe_name']).sale_price.shift(3)\n",
    "sales_history['pct_vs_last_3'] = sales_history['sale_lagged_2'] / sales_history['sale_lagged_3'] - 1\n",
    "# Sale price minus 4\n",
    "sales_history['sale_lagged_4'] = sales_history.groupby(['shoe_name']).sale_price.shift(4)\n",
    "sales_history['pct_vs_last_4'] = sales_history['sale_lagged_3'] / sales_history['sale_lagged_4'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: computing rolling average requires unique index of name + datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_history = sales_history.set_index(['name', 'sale_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rolling average\n",
    "sales_history['rolling_avg_4'] = sales_history.groupby(level = 0)['sale_price'].rolling(4).mean().shift(1).reset_index(0,drop=True)\n",
    "# sales_history['rolling_avg_20'] = sales_history.groupby(level = 0)['sale_price'].rolling(20).mean().shift(1).reset_index(0,drop=True)\n",
    "\n",
    "# Insert an intercept\n",
    "sales_history['intercept'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the lagged sales are calculated properly\n",
    "sales_history.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then drop the NaNs.\n",
    "sales_history = sales_history.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the other variables into logs\n",
    "sales_history['sale_lagged_1'] = np.log(sales_history.sale_lagged_1)\n",
    "sales_history['sale_lagged_2'] = np.log(sales_history.sale_lagged_2)\n",
    "sales_history['sale_lagged_3'] = np.log(sales_history.sale_lagged_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_names = list(sales_history.columns.values)\n",
    "len(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "# Model for all shoes\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_shoes = sales_history.reset_index()\n",
    "all_shoes = all_shoes.dropna()\n",
    "all_shoes = all_shoes.sort_values(['sale_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the min and max of rolling average for the forecast function later\n",
    "rolling_avg_4_min = all_shoes.rolling_avg_4.min()\n",
    "rolling_avg_4_max = all_shoes.rolling_avg_4.max()\n",
    "# rolling_avg_20_min = all_shoes.rolling_avg_20.min()\n",
    "# rolling_avg_20_max = all_shoes.rolling_avg_20.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize the team_stats dataframe between 0 and 1 (between -1 and 1 would be team_stats.mean()/ same as below)\n",
    "all_shoes['rolling_avg_4'] = (all_shoes['rolling_avg_4'] - all_shoes.rolling_avg_4.min()) / (all_shoes.rolling_avg_4.max() - all_shoes.rolling_avg_4.min())\n",
    "# all_shoes['rolling_avg_20'] = (all_shoes['rolling_avg_20'] - all_shoes.rolling_avg_20.min()) / (all_shoes.rolling_avg_20.max() - all_shoes.rolling_avg_20.min())\n",
    "\n",
    "# all_shoes['rolling_avg_4'] = np.log(all_shoes.rolling_avg_4)\n",
    "# all_shoes['rolling_avg_20'] = np.log(all_shoes.rolling_avg_20)\n",
    "all_shoes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_shoes.loc[all_shoes.name == 'Adidas NMD R1 Black Red', ['sale_price', 'rolling_avg_4']].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the dummy variabls for the non-numerical features\n",
    "for col in all_shoes[['main_color','brand','line']]:\n",
    "    dummies = pd.get_dummies(all_shoes[col], prefix = col)\n",
    "    all_shoes = all_shoes.join(dummies)\n",
    "    all_shoes = all_shoes.drop(col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop the nas again?\n",
    "all_shoes = all_shoes.dropna()\n",
    "all_shoes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_shoes = all_shoes.replace([np.inf, -np.inf], np.nan)\n",
    "all_shoes = all_shoes.dropna()\n",
    "all_shoes = all_shoes.reset_index()\n",
    "\n",
    "\n",
    "# Desired outcome variable\n",
    "y = all_shoes.log_sale_price\n",
    "\n",
    "# Full set of features - no shoe size anymore\n",
    "X = all_shoes[['intercept', 'rolling_avg_4','pct_vs_last_2','pct_vs_last_3','log_total_days_td',\n",
    "              'main_color_basic', 'original_retail','brand_nike','line_foamposite','line_air']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set test train split\n",
    "cutpoint = round((len(X)/4)*3)\n",
    "\n",
    "X_train = X.ix[:cutpoint,:]\n",
    "X_test = X.ix[cutpoint:,:]\n",
    "\n",
    "y_train = y.ix[:cutpoint]\n",
    "y_test = y.ix[cutpoint:]\n",
    "\n",
    "model = sm.OLS(y_train, X_train)\n",
    "est = model.fit()\n",
    "y_train_pred = est.predict(X_train)\n",
    "y_test_pred = est.predict(X_test)\n",
    "r_2 = est.rsquared\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate means of y_test and y_train\n",
    "mean_y_test = np.mean(y_test)\n",
    "mean_y_train = np.mean(y_train)\n",
    "\n",
    "# print r_squared (train and test)\n",
    "print(1 - (mean_squared_error(y_train, y_train_pred)/mean_squared_error([mean_y_train]*len(y_train), y_train)))\n",
    "print(1 - (mean_squared_error(y_test, y_test_pred)/mean_squared_error([mean_y_test]*len(y_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = RandomForestRegressor(n_estimators=1000, max_depth=5)\n",
    "\n",
    "tree_model = tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tree_pred = tree_model.predict(X_train)\n",
    "test_tree_pred = tree_model.predict(X_test)\n",
    "all_preds = tree_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print r_squared (train and test)\n",
    "print(1 - (mean_squared_error(y_train, train_tree_pred)/mean_squared_error([mean_y_train]*len(y_train), y_train)))\n",
    "print(1 - (mean_squared_error(y_test, test_tree_pred)/mean_squared_error([mean_y_test]*len(y_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, test_tree_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import forestci as fci\n",
    "# http://contrib.scikit-learn.org/forest-confidence-interval/auto_examples/plot_mpg.html#sphx-glr-auto-examples-plot-mpg-py\n",
    "# calculate inbag and unbiased variance\n",
    "# inbag = fci.calc_inbag(X_train.shape[0], tree)\n",
    "# unbiased = fci.random_forest_error(tree, inbag, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluate test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get predicted and actual Y values from test data set\n",
    "# Actual price\n",
    "y_test2 = y_test.reset_index(drop = True)\n",
    "print(y_test2.tail(1))\n",
    "# Predicted price\n",
    "yhat = pd.Series(test_tree_pred)\n",
    "print(yhat.tail(1))\n",
    "# Shoe and date info\n",
    "names_dates = all_shoes.ix[cutpoint:, ['name', 'date']].reset_index(drop = True)\n",
    "print(names_dates.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How well does the model predict if the price will go up or down? \n",
    "testing = pd.concat([names_dates, y_test2, yhat], axis = 1)\n",
    "testing.columns = ['name', 'date', 'y', 'yhat']\n",
    "testing.y = np.exp(testing.y)\n",
    "testing.yhat = np.exp(testing.yhat)\n",
    "testing = testing.set_index(['name', 'date'])\n",
    "testing = testing.sort_index()\n",
    "testing = testing.reset_index()\n",
    "# Lagged y and yhat\n",
    "testing['y_last'] = testing.groupby(['name']).y.shift(1)\n",
    "testing['yhat_last'] = testing.groupby(['name']).yhat.shift(1)\n",
    "# Compare both y and yhat to previous y value\n",
    "testing['y_change'] = testing.y - testing.y_last\n",
    "testing['yhat_change'] = testing.yhat - testing.y_last ## less accurate if you use yhat last\n",
    "# Flags for increase in y \n",
    "testing['y_up'] = 0\n",
    "testing.loc[testing.y_change >= 0, 'y_up'] = 1\n",
    "testing['yhat_up'] = 0\n",
    "testing.loc[testing.yhat_change >= 0, 'yhat_up'] = 1\n",
    "testing = testing.dropna()\n",
    "# Summary of results\n",
    "testsum = testing.groupby(['y_up', 'yhat_up']).size()\n",
    "testsum = pd.DataFrame(testsum).reset_index()\n",
    "testsum.columns = ['y_up', 'yhat_up', 'n'] \n",
    "pct_num = (testsum.loc[0, 'n'] + testsum.loc[3, 'n'])\n",
    "pct_denom = testsum['n'].sum()\n",
    "pct_correct = round(float(pct_num) / float(pct_denom), 3)\n",
    "print(testsum)\n",
    "print(pct_num)\n",
    "print(pct_denom)\n",
    "print(pct_correct)\n",
    "testing.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions for the flask app\n",
    "# Shoe list for forecast\n",
    "shoe_list = [i for i in all_shoes['name'].unique()]\n",
    "shoe_n = len(shoe_list)\n",
    "\n",
    "# have a random shoe to check whenever i want to check a shoe\n",
    "chosen_shoe = shoe_list[randint(0, shoe_n)]\n",
    "\n",
    "# Data set for forecast - all the x variables but also name, sale price, pct vs last\n",
    "forecast = all_shoes[['name', 'sale_price', 'pct_vs_last', 'intercept', 'rolling_avg_4','pct_vs_last_2','pct_vs_last_3','log_total_days_td',\n",
    "              'main_color_basic', 'original_retail','brand_nike','line_Foamposite','line_Air']]\n",
    "\n",
    "# Choose a shoe for one run\n",
    "x = forecast[forecast.name == chosen_shoe].reset_index(drop = True)\n",
    "x.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_new_row(df, mod):\n",
    "    \"\"\" Forecast the sale price of the next sale using the model \"\"\"\n",
    "    # The last row of the dataframe & sale_price\n",
    "    last_row = (len(df) - 1)\n",
    "    last_sale_price = df.sale_price[last_row]\n",
    "    \n",
    "    # the row that will be created\n",
    "    new_row_num = len(df)\n",
    "\n",
    "    # Generate X values for next observation\n",
    "    rolling_avg_4 = df.sale_price[(last_row - 3):(last_row + 1)].mean() # adjust for rolling window\n",
    "#     rolling_avg_20 = df.sale_price[(last_row - 19):(last_row + 1)].mean() # adjust for rolling window\n",
    "\n",
    "    # Normalize rolling average using global variables created above \n",
    "    rolling_avg_4 = (rolling_avg_4 - rolling_avg_4_min) / (rolling_avg_4_max - rolling_avg_4_min)\n",
    "    # rolling_avg_20 = (rolling_avg_20 - rolling_avg_20_min) / (rolling_avg_20_max - rolling_avg_20_min)\n",
    "    \n",
    "    # take pct vs last from row above\n",
    "    pct_vs_last_2 = df.pct_vs_last[last_row]\n",
    "    pct_vs_last_3 = df.pct_vs_last_2[last_row]\n",
    "    \n",
    "    # added 1 day to the time delta and then re log it \n",
    "    log_total_days_td = np.log(np.exp(df.log_total_days_td[last_row]) + 1) # better way to do this? \n",
    "    \n",
    "    # fill in dataframe with other features by shoe\n",
    "    main_color_basic = df.main_color_basic[last_row]\n",
    "    original_retail = df.original_retail[last_row]\n",
    "    brand_nike = df.brand_nike[last_row]\n",
    "    line_Foamposite = df.line_Foamposite[last_row]\n",
    "    line_Air = df.line_Air[last_row]\n",
    "    \n",
    "    # add the intercept\n",
    "    intercept = 1\n",
    "    \n",
    "    # New array with all the variables saved above\n",
    "    new_row = [intercept, rolling_avg_4, pct_vs_last_2, pct_vs_last_3, log_total_days_td, \n",
    "               main_color_basic, original_retail, brand_nike, line_Foamposite, line_Air]\n",
    "    \n",
    "    # convert new row to array and reshape\n",
    "    new_array = np.asarray(new_row)\n",
    "    new_array = new_array.reshape(1, -1) # to avoid numpy deprecation warning\n",
    "    \n",
    "    # Predict w/ tree model\n",
    "    log_pred_value = tree_model.predict(new_array)\n",
    "    pred_value = float(np.exp(log_pred_value)[0])\n",
    "    \n",
    "    # fill in sale price for new row, calc pct vs last on predicted sale price\n",
    "    sale_price = pred_value # sample from distribution? \n",
    "    pct_vs_last = sale_price / df.sale_price[last_row] - 1\n",
    "    \n",
    "    # adding everything together to make the new row \n",
    "    post_pred = [df.name[last_row], sale_price, pct_vs_last]\n",
    "    new_row_for_df = post_pred + new_row\n",
    "    df.ix[new_row_num] = new_row_for_df\n",
    "    \n",
    "    return pred_value, last_sale_price, df\n",
    "\n",
    "# run it for the one shoe\n",
    "pred_value, last_sale_price, newdf = make_new_row(x, tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Forecast next sale for all shoes! \n",
    "pred_results = []\n",
    "for i in shoe_list:\n",
    "    # subset dataframe to i in shoe list\n",
    "    xdf = forecast[forecast.name == i].reset_index(drop = True)\n",
    "    # next 7 days\n",
    "    for n in range(0, 7):\n",
    "        n_label = n + 1\n",
    "        pred_value, last_sale_price, xdf = make_new_row(xdf, tree_model)\n",
    "        pred_results.append([i, n_label, pred_value, last_sale_price])\n",
    "pred_results = pd.DataFrame(pred_results)\n",
    "pred_results.columns = ['name', 'n', 'predicted', 'last_sale_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chosen_shoe = shoe_list[randint(0, shoe_n)]\n",
    "pred_results.loc[pred_results.name == chosen_shoe, ['predicted']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the mean of predictions for each shoe\n",
    "pred2 = pred_results.copy()\n",
    "pred2['avg_pred'] = pred2.groupby('name')['predicted'].transform('mean')\n",
    "pred2 = pred2[pred2.n == 1]\n",
    "pred2 = pred2[['name', 'avg_pred', 'last_sale_price']]\n",
    "pred2.columns = ['name', 'predicted', 'last_sale_price']\n",
    "pred2.predicted = pred2['predicted'].apply(lambda x: round(x, 2))\n",
    "pred2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add the up or down column and send to csv for the flask app\n",
    "pred2['change'] = pred2['predicted'] - pred2['last_sale_price']\n",
    "pred2['trend'] = 'down'\n",
    "pred2.loc[pred2.change > 0, 'trend'] = 'up'\n",
    "pred2.to_csv('shoe_forecast.csv', index = False)\n",
    "pred2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred2.trend.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
